---
title: "Attention"
date: 2020-01-12T01:09:07+11:00
draft: true
---

Attention mechanism [1,2] improved NLP architectures by allowing them to focus on a relevant part of input/representation similar to how we humans do. While reading a text if the first and last character of a word is correct, humans can understand the text [3]. This post examines the inner working of additive and multiplicative attention, i.e. How attention mechanism converts query and hidden states into attention scores.

<!--more-->

## Introduction and Motivation
The attention enables model to focus on the relevant part of input at a given timestamp. We humans use the attention mechanism in our daily life for pretty much everything. When trying to answer a question based on comprehension passage, we look for important sentences to answer the question similar to attention mechanism. The attention mechanism was introduced by Bahdanau[2] in sequence to sequence models used for translation. While generating translation decoder will focus on parts of input which are relevant to word being generated. It solved the issue of bottleneck of using only final state of encoder in sequence to sequence models and also resulted in model being able to handle longer sentence in translations.

There are various type of attention mechanisms and a good summary is available on Lillian Weng's [Blog](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#summary) [3], We will only discuss additive and dot product attention and their implementations.

## Basic components

Three basic ingredients in attention are:

* **Query** : The query provides the context into what we are currently after to decide where to focus in sentence. In seq2seq model, decoder state from last time step is used as query to guide what is generated next.

* **Key** : The key can be multiple states generated by seq2seq encoder model which will be matched with query while decoding.

* **Value** : Value represents actual value which can be selected. This can be same as key and mostly is. 

In most of the cases keys will refer to the multiple available states by encoder.
## Additive Attention (Bahdanau)

Additive attention use the decoder state from last time step as query(s_t) and keys are encoder states(h_t) for the whole sequence. This mechanism adds query to each of encoder state and hence the name additive attention.

This procedure in Bahdanau's attention mechanism is mathematically shown as
$$
query =  W_q.s_t
$$
$$
keys = [W_k.h_i \quad i \in [1 \dotsc T]]
$$
$$
scores = v_a.tanh(query + key)
$$

### Transform Query and hidden states using projection layer and merge
We first transform query and hidden state using a weight embedding (linear layer) to similar dimension. This step is important because query and hidden states might have different dimension so to compare both of them we need to transform into a similar space using projection.

Define the  Sizes of input key, Query, values
```python
# Imports
import torch
import torch.nn as nn

# Size of hidden dimension of model.
hidden_dim = 128
# Size of encoder, considering encoder was bi-directional.
key_size = 2*hidden_dim
# Decoder is not birectional hence not multiplied by 2.
query_size = hidden_dim
value_size = hidden_dim
batch_size = 64
max_seq = 20
```

Define the dummy key and query and projection layers.
```python
## Using the dummy key, query, values
key = nn.randn(batch_size, max_seq, key_size)
query = nn.randn(batch_size, query_size)

## Define projection layer to map key and query to same dimention
key_layer = nn.Linear(key_size, hidden_dim, bias=False)
query_layer = nn.Linear(query_size, hidden_dim, bias=False)
```

The query dimension are (batch_size, query_size) where as keys are (batch_size, max_seq, key_size). We need to expand the dimension of query to account for sequence lengths which is done using unsqueeze function.

```python
## torch.size([64, 128]) => torch.size([64, 1, 128])
query.unsqueeze(1)

# We need to 
## Doing the actual projection on input
proj_key = key_layer(key)
proj_query = query_layer(query) 

# Pass it through non-linearity
projected_input = torch.tanh(proj_key + proj_query)
```

Above is the original implementation by Bahdanau [2] but we can also implement it by concatinating the key and query together and then passing through linear layer which is better as it allows finer-grain merging of states from query and key. Hence better matching and attention scores.
$$
W_a[s_t;h_i] \quad i \in [1 \dotsc T]
$$
```python3
# Keys shape is batch_sizeXmax_seqXhidden_dim 
key = torch.randn(batch_size, max_seq, hidden_dim)

proj = nn.Linear(2*hidden_dim, hidden_dim) 

# Query shape is batch_sizeXhidden_dim
# Add extra dimension for sequence length to query
# batch_sizeXhidden_dim => batch_sizeX1Xhidden_dim
query = query.unsqueeze(1)

# batch_sizeX1Xhidden_dim => batch_sizeXmax_seqXhidden_dim
query = query.expand_as(key)

# Concatinate Query and Keys
merged = torch.cat((key, query), dim=2)
# Now shape of merged is batch_sizeXmax_seqX(2*hidden_dim)

# use projection layet o project to input
projected_input = proj(merged)
projected_input = torch.tanh(projected_input)
```
## Project down the merged representation and calculate attention scores
Above step provides us with matched representation of query from decoder and keys from encoder but this is not attention score are values between 0 and 1 reflecting the contribution of each key at current time. This is implemented using another projection referred by v_a which project down the hidden state and and then passing through softmax layer to generate probability distribution on keys.

Mathematically this projects the representation further
$$
v_a.\mathrm{tanh}(W_a[s_t;h_i]) \quad  i \in [1 \dotsc T]
$$

```python3
# Projection to lower dimension
enery_layer = nn.Linear(hidden_dim, 1)

# Output of energy is batch_sizeXmax_seqX1
energy = enery_layer(projected_input)
# Dimension of energy are 64X20X1

# Since attention is probability distribution across all states.
# We convert it to dimension batch_sizeX1Xmax_seq so that multiplying
# attention scores to value of states gives us correct state formed after applying attention
energy = energy.squeeze(2).unsqueeze(1)
# energy  is are not 64X1X20

# Pass through softmax layer
attention_scores = F.softmax(energy, dim=-1)
#alphas is still 64X1X20 but now all values sums to 1.

attended_state = attorch.bmm(attention_scores, values)
# attention shape is now 64X1X128
```

## Self-Attention (Vaswani)

## Reference
* [1] Vaswani, Ashish, et al. "Attention is all you need." Advances in neural information processing systems. 2017.
* [2] Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate." arXiv preprint arXiv:1409.0473 (2014).
* [3] https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html
* [4] https://www.foxnews.com/story/if-you-can-raed-tihs-you-msut-be-raelly-smrat

